<html>
	<head>
		<title>	CV_KirtiThakkar</title>
	</head>
<body>
	
	
	<div style="width:70%;Float:left">
		<h1 align="center"><b>Resume<b></h1>
	
		<h3 align="left"> CONTACT </h3>
		<p align="center"> <b>Kirti Thakkar</b> </p>
		<p align="center"> <b>+91-8200884791</b> </p>
		<p align="center"> <b>kirtithakkar2807@gmail.com</b> </p>
	</div>
	<div style="width=30%;Float:left">
	</div>
	
	
	
	
	
	
	
	
	
	<!--
	<hr>
	<h3 align="left"><b>OBJECTIVE:<b></h3>
	<P> Dedicated Data Analyst at S$P Global, seeking a challanging role in a reputable Corporation where I can leverage my expertise in data analysis, web crawling and scrapping, and visualization to drive business growth and informed decision-making, utilizing skills in Data Analysis, Research, Data Management, Audit work and Trainings, Excel and C programming to deliver actionable insights and strategic recommendations.
	
	<hr>
	<h3 align="left"><b>Experience:<b></h3>
	<p align="left"> 1 March 2024-Present</p>
	<Pre>
	<b>-Data Analyst</b>
	    As a seasoned data analyst, I specialize in managing a robust generic web crawler process and Mozenda tool designed to extract financial and non-financial documents from company URLs worldwide. My expertise lies in implementing advanced logic algorithms and leveraging backend data to automate the downloading of essential documents, ensuring our repository remains up-to-date.
			Key Responsibilities:
				1. *Web Crawler & Mozenda Management*: Design, implement, and maintain web crawlers and Mozenda agents to extract relevant financial and non-financial documents from company URLs globally.
				2. *Automation*: Utilize advanced logic algorithms and backend and frontend data to automate document downloading, streamlining the update process for our repository.
				3. *Quality Assurance*: Conduct thorough quality checks on entities created by other users and interns to ensure data accuracy and consistency.
				4. *URL Audit*: Perform audits on URLs to eliminate duplicates, ensuring efficient job creation and minimizing unnecessary crawls.
				5. *Repository Maintenance*: Continuously update and refine our repository to provide clients with accurate and reliable data.
			Technical Expertise:
				- Advanced logic algorithms
				- Web crawling technologies
				- Data automation
				- Data quality control
				- Backend and frontend data management
		
		
		Also,Utilize expertise in research and data analysis to gather critical documents for private companies across 50 US states, ensuring comprehensive and accurate data collection.
			*Key Responsibilities:
				- Conduct in-depth research to gather Annual Reports, Certificates of Incorporation, M&A documents, and other filings
				- Leverage manual search techniques and BOT technology for efficient data collection
				- Validate and upload documents using CTS and Bulk Uploader systems
				- Maintain up-to-date repository of verified information
			*Skills:
				- Data Research and Analysis
				- Document Validation and Management
				- Automation Technology (BOT)
				- Data Upload and Management (CTS, Bulk Uploader)

</pre>
	
	-->
	
	
</body>
		
</html>